{
 "cells": [],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "# Copyright [2020] [KTH Royal Institute of Technology] Licensed under the\n",
     "# Educational Community License, Version 2.0 (the \"License\"); you may\n",
     "# not use this file except in compliance with the License. You may\n",
     "# obtain a copy of the License at http://www.osedu.org/licenses/ECL-2.0\n",
     "# Unless required by applicable law or agreed to in writing,\n",
     "# software distributed under the License is distributed on an \"AS IS\"\n",
     "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n",
     "# or implied. See the License for the specific language governing\n",
     "# permissions and limitations under the License.\n",
     "#\n",
     "# Course: EL2805 - Reinforcement Learning - Lab 2 Problem 1\n",
     "# Code author: [Alessio Russo - alessior@kth.se]\n",
     "# Last update: 6th October 2020, by alessior@kth.se\n",
     "#\n",
     "\n",
     "import numpy as np\n",
     "import gym\n",
     "import torch\n",
     "from torch import nn\n",
     "import matplotlib.pyplot as plt\n",
     "from matplotlib import cm\n",
     "from mpl_toolkits.mplot3d import Axes3D\n",
     "from tqdm import trange\n",
     "from DQN_agent import RandomAgent, EpsGreedyAgent\n",
     "import random\n",
     "from collections import namedtuple\n",
     "\n",
     "\n",
     "def running_average(x, N):\n",
     "    ''' Function used to compute the running average\n",
     "        of the last N elements of a vector x\n",
     "    '''\n",
     "    if len(x) >= N:\n",
     "        y = np.copy(x)\n",
     "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
     "    else:\n",
     "        y = np.zeros_like(x)\n",
     "    return y\n",
     "\n",
     "\n",
     "# Import and initialize the discrete Lunar Laner Environment\n",
     "env = gym.make('LunarLander-v2')\n",
     "env.reset()\n",
     "\n",
     "# Parameters\n",
     "n_episodes = 300                             # Number of episodes\n",
     "discount_factor = 0.95                       # Value of the discount factor\n",
     "n_ep_running_average = 50                    # Running average of 50 episodes\n",
     "n_actions = env.action_space.n               # Number of available actions\n",
     "dim_state = len(env.observation_space.high)  # State dimensionality\n",
     "\n",
     "# We will use these variables to compute the average episodic reward and\n",
     "# the average number of steps per episode\n",
     "episode_reward_list = []       # this list contains the total reward per episode\n",
     "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
     "\n",
     "# Random agent initialization\n",
     "# agent = RandomAgent(n_actions)\n",
     "print(env.observation_space)\n",
     "print(env.observation_space.high)\n",
     "print(dim_state)\n",
     "print(n_actions)\n",
     "hidden_size = 128\n",
     "\n",
     "def DQN(hidden_size):\n",
     "    return nn.Sequential(\n",
     "        nn.Linear(dim_state, hidden_size),\n",
     "        nn.ReLU(),\n",
     "        nn.Linear(hidden_size, hidden_size),\n",
     "        nn.ReLU(),\n",
     "        nn.Linear(hidden_size, hidden_size),\n",
     "        nn.ReLU(),\n",
     "        nn.Linear(hidden_size, n_actions)\n",
     "    )\n",
     "\n",
     "\n",
     "Transition = namedtuple('Transition',\n",
     "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
     "\n",
     "\n",
     "class ReplayMemory(object):\n",
     "\n",
     "    def __init__(self, capacity):\n",
     "        self.capacity = capacity\n",
     "        self.memory = []\n",
     "        self.position = 0\n",
     "\n",
     "    def push(self, *args):\n",
     "        \"\"\"Saves a transition.\"\"\"\n",
     "        if len(self.memory) < self.capacity:\n",
     "            self.memory.append(None)\n",
     "        self.memory[self.position] = Transition(*args)\n",
     "        self.position = (self.position + 1) % self.capacity\n",
     "\n",
     "    def sample(self, batch_size):\n",
     "        return random.sample(self.memory, batch_size)\n",
     "\n",
     "    def __len__(self):\n",
     "        return len(self.memory)\n",
     "\n",
     "EPISODES = trange(n_episodes, desc='Episode: ', leave=True)\n",
     "render_period = 150\n",
     "\n",
     "loss_fn = torch.nn.MSELoss()\n",
     "\n",
     "policy_net = DQN(hidden_size)\n",
     "target_net = DQN(hidden_size)\n",
     "target_net.load_state_dict(policy_net.state_dict())\n",
     "target_net.eval()\n",
     "\n",
     "agent = EpsGreedyAgent(n_actions, n_episodes, policy_net)\n",
     "\n",
     "BATCH_SIZE = 128\n",
     "BUFFER_SIZE = 20000\n",
     "TARGET_UPDATE = BUFFER_SIZE // BATCH_SIZE\n",
     "LEARNING_RATE = 2e-04\n",
     "\n",
     "memory = ReplayMemory(BUFFER_SIZE)\n",
     "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
     "t = 0\n",
     "for episode in EPISODES:\n",
     "    # Reset enviroment data and initialize variables\n",
     "    done = False\n",
     "    state = torch.from_numpy(env.reset())\n",
     "    total_episode_reward = 0.\n",
     "    while not done:\n",
     "        if episode % render_period == 0:\n",
     "            env.render()\n",
     "\n",
     "        # Take action\n",
     "        action = agent.forward(state, episode)\n",
     "\n",
     "        next_state, reward, done, _ = env.step(action.numpy()[0][0])\n",
     "        next_state = torch.from_numpy(next_state)\n",
     "\n",
     "        # Store the transition in memory\n",
     "        memory.push(state,\n",
     "            action, next_state, torch.tensor([reward], dtype=torch.float), done)\n",
     "\n",
     "        total_episode_reward += reward\n",
     "\n",
     "        # Update state for next iteration\n",
     "        state = next_state\n",
     "        t += 1\n",
     "        # Filling up buffer\n",
     "        if len(memory) < BATCH_SIZE*4:\n",
     "            continue\n",
     "        transitions = memory.sample(BATCH_SIZE)\n",
     "        batch = Transition(*zip(*transitions))\n",
     "\n",
     "        # Compute a mask of non-final states and concatenate the batch elements\n",
     "        # (a final state would've been the one after which simulation ended)\n",
     "        non_final_mask = torch.logical_not(torch.tensor(batch.done))\n",
     "        next_state_batch = torch.cat(batch.next_state).reshape((BATCH_SIZE, -1))\n",
     "        state_batch = torch.cat(batch.state).reshape((BATCH_SIZE, -1))\n",
     "        action_batch = torch.cat(batch.action).reshape((BATCH_SIZE, -1))\n",
     "        reward_batch = torch.cat(batch.reward).reshape((BATCH_SIZE, -1))\n",
     "\n",
     "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
     "        # columns of actions taken. These are the actions which would've been taken\n",
     "        # for each batch state according to policy_net\n",
     "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
     "\n",
     "        # Compute V(s_{t+1}) for all next states.\n",
     "        # Expected values of actions for non_final_next_states are computed based\n",
     "        # on the \"older\" target_net; selecting their best reward with max(1).values.\n",
     "        # This is merged based on the mask, such that we'll have either the expected\n",
     "        # state value or 0 in case the state was final.\n",
     "        next_state_values = torch.zeros(BATCH_SIZE)\n",
     "        next_state_pred = target_net(\n",
     "            next_state_batch).max(1).values\n",
     "        next_state_values[non_final_mask] = next_state_pred[non_final_mask]\n",
     "        # Compute the expected Q values\n",
     "        expected_state_action_values = (\n",
     "            next_state_values * discount_factor) + reward_batch.squeeze()\n",
     "\n",
     "        loss = loss_fn(\n",
     "            state_action_values, expected_state_action_values.unsqueeze(1))\n",
     "\n",
     "        optimizer.zero_grad()\n",
     "\n",
     "        loss.backward()\n",
     "        torch.nn.utils.clip_grad_norm(policy_net.parameters(), 1)\n",
     "\n",
     "        optimizer.step()\n",
     "\n",
     "        # Update the target network, copying all weights and biases in DQN\n",
     "        if t % TARGET_UPDATE == 0:\n",
     "            target_net.load_state_dict(policy_net.state_dict())\n",
     "\n",
     "    # Append episode reward and total number of steps\n",
     "    episode_reward_list.append(total_episode_reward)\n",
     "    episode_number_of_steps.append(t)\n",
     "\n",
     "    # Close environment\n",
     "    env.close()\n",
     "\n",
     "    # Updates the tqdm update bar with fresh information\n",
     "    # (episode number, total reward of the last episode, total number of Steps\n",
     "    # of the last episode, average reward, average number of steps)\n",
     "    EPISODES.set_description(\n",
     "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
     "            episode, total_episode_reward, t,\n",
     "            running_average(episode_reward_list, n_ep_running_average)[-1],\n",
     "            running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
     "\n",
     "    if running_average(episode_reward_list, n_ep_running_average)[-1] > 51:\n",
     "        print(\"Good enough! Stopping learning\")\n",
     "        break\n",
     "\n",
     "torch.save(policy_net, 'neural-network-1.pth')\n",
     "\n",
     "# Plot Rewards and steps\n",
     "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
     "ax[0].plot([i for i in range(1, n_episodes+1)],\n",
     "           episode_reward_list, label='Episode reward')\n",
     "ax[0].plot([i for i in range(1, n_episodes+1)], running_average(\n",
     "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
     "ax[0].set_xlabel('Episodes')\n",
     "ax[0].set_ylabel('Total reward')\n",
     "ax[0].set_title('Total Reward vs Episodes')\n",
     "ax[0].legend()\n",
     "ax[0].grid(alpha=0.3)\n",
     "\n",
     "ax[1].plot([i for i in range(1, n_episodes+1)],\n",
     "           episode_number_of_steps, label='Steps per episode')\n",
     "ax[1].plot([i for i in range(1, n_episodes+1)], running_average(\n",
     "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
     "ax[1].set_xlabel('Episodes')\n",
     "ax[1].set_ylabel('Total number of steps')\n",
     "ax[1].set_title('Total number of steps vs Episodes')\n",
     "ax[1].legend()\n",
     "ax[1].grid(alpha=0.3)\n",
     "plt.show()\n",
     "\n",
     "# (f)\n",
     "\n",
     "angles = np.arange(-np.pi, np.pi, 0.1)\n",
     "heights = np.arange(0, 1.5, 0.05)\n",
     "q_values = np.zeros((len(heights), len(angles)))\n",
     "\n",
     "for i, angle in enumerate(angles):\n",
     "    for j, height in enumerate(heights):\n",
     "        state = torch.tensor((0, height, 0, 0, angle, 0, 0, 0))\n",
     "        net_input = torch.unsqueeze(state, 0)\n",
     "        action_value = policy_net(net_input).max(1).values[0]\n",
     "        q_values[j, i] = action_value\n",
     "\n",
     "fig = plt.figure()\n",
     "ax = fig.gca(projection='3d')\n",
     "\n",
     "X, Y = np.meshgrid(angles, heights)\n",
     "surf = ax.plot_surface(X, Y, q_values, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
     "plt.show()"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}